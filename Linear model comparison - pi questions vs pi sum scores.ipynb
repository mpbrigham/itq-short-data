{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model comparison - pi questions vs pi sum scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import qgrid\n",
    "import plotly.express as px\n",
    "\n",
    "import mod_evaluation\n",
    "\n",
    "fig_w, fig_h = (4.5, 3.5)\n",
    "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = 'data/source'\n",
    "results_folder = 'data/results'\n",
    "\n",
    "# data_file = '181213_ITQvar.csv'\n",
    "# data_path = os.path.join(data_folder, data_file)\n",
    "\n",
    "# data_file_hold = '190606_holdout_validation_data_PI_items_rescaled.csv'\n",
    "# data_path_hold = os.path.join(data_folder, data_file_hold)\n",
    "\n",
    "# data_file_map ='MappingNamesSumScores.csv'\n",
    "# data_path_map = os.path.join(data_folder, data_file_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sumscores = mod_data.open_df_sumscores(data_path)\n",
    "# df_questions = mod_data.open_df_questions(data_path)\n",
    "\n",
    "# df_hold_sumscores = mod_data.open_df_hold_sumscores(data_path_hold)\n",
    "# df_hold_questions = mod_data.open_df_hold_questions(data_path_hold, data_path_map)\n",
    "\n",
    "# assert(set(df_questions.columns)==set(df_hold_questions.columns))\n",
    "# assert(set(df_sumscores.columns)==set(df_hold_sumscores.columns))\n",
    "\n",
    "# print('Dataset sum scores')\n",
    "# display(df_sumscores[:3])\n",
    "\n",
    "# print('Dataset questions')\n",
    "# display(df_questions[:3])\n",
    "\n",
    "# print('Holdout sum scores')\n",
    "# display(df_hold_sumscores[:3])\n",
    "\n",
    "# print('Holdout set questions')\n",
    "# display(df_hold_questions[:3])\n",
    "\n",
    "# df_train = df_questions\n",
    "# df_val = df_hold_questions\n",
    "\n",
    "# y_name = 'logits'\n",
    "# y_train, y_logits_train = mod_latent.modelPP(np.array(df_sumscores))\n",
    "# y_val, y_logits_val = mod_latent.modelPP(np.array(df_hold_sumscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ref = mod_evaluation.model_linear\n",
    "# model_ref_kwargs = { }\n",
    "# features_quad = True\n",
    "\n",
    "# metric_sort = 'val_mean_squared_error'\n",
    "# metric_filter = ('val_categorical_accuracy', 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14c6c09dd14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquestions_info_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'questions_drop_pi_info_best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestions_stats_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'questions_drop_pi_stats_best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msum_score_stats_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum_score_drop_pi_stats_multi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mod_utils' is not defined"
     ]
    }
   ],
   "source": [
    "questions_info_best = mod_evaluation.from_cache('questions_drop_pi_info_best', results_folder)\n",
    "\n",
    "questions_stats_best = mod_evaluation.from_cache('questions_drop_pi_stats_best', results_folder)\n",
    "\n",
    "sum_score_stats_multi = mod_evaluation.from_cache('sum_score_drop_pi_stats_multi', results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_stats_m = mod_evaluation.stats_eval(questions_stats_best)\n",
    "\n",
    "questions_stats_ci = mod_evaluation.stats_eval(\n",
    "    questions_stats_best, \n",
    "    fn=lambda a,**kwargs: mod_evaluation.bootstrap_ci(a, alpha=10, **kwargs),\n",
    "    selected=[\n",
    "        'categorical_accuracy','val_categorical_accuracy',\n",
    "        'mean_squared_error','val_mean_squared_error'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sum_score_stats_multi_m = {\n",
    "    key: mod_evaluation.stats_eval(\n",
    "        sum_score_stats_multi[key][0]\n",
    "    )\n",
    "    for key in sum_score_stats_multi\n",
    "}\n",
    "sum_score_stats_multi_ci = {\n",
    "    key: mod_evaluation.stats_eval(\n",
    "        sum_score_stats_multi[key][0], \n",
    "        fn=lambda a,**kwargs: mod_evaluation.bootstrap_ci(a, alpha=10, **kwargs),\n",
    "        selected=[\n",
    "            'categorical_accuracy','val_categorical_accuracy',\n",
    "            'mean_squared_error','val_mean_squared_error'\n",
    "        ]\n",
    "    )\n",
    "    for key in sum_score_stats_multi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_questions():\n",
    "    df = pd.DataFrame([\n",
    "        [model_id, questions_info_best[model_id]['drop_n'], 17-questions_info_best[model_id]['x_d']]\n",
    "        + [questions_stats_ci[model_id]['categorical_accuracy'][0], questions_stats_m[model_id]['categorical_accuracy'], questions_stats_ci[model_id]['categorical_accuracy'][1]]\n",
    "        + [questions_stats_ci[model_id]['val_categorical_accuracy'][0], questions_stats_m[model_id]['val_categorical_accuracy'], questions_stats_ci[model_id]['val_categorical_accuracy'][1]]\n",
    "        + [questions_stats_ci[model_id]['mean_squared_error'][0], questions_stats_m[model_id]['mean_squared_error'], questions_stats_ci[model_id]['mean_squared_error'][1]]\n",
    "        + [questions_stats_ci[model_id]['val_mean_squared_error'][0], questions_stats_m[model_id]['val_mean_squared_error'], questions_stats_ci[model_id]['val_mean_squared_error'][1]]    \n",
    "        for model_id in natsorted(questions_stats_m)\n",
    "    ])\n",
    "\n",
    "    df.columns = [\n",
    "        'model', 'drop_q', 'drop_sum',\n",
    "        'acc_l', 'acc_m','acc_u',\n",
    "        'val_acc_l','val_acc_m','val_acc_u',\n",
    "        'mse_l', 'mse_m', 'mse_u',\n",
    "        'val_mse_l', 'val_mse_m', 'val_mse_u'\n",
    "    ]\n",
    "    df.set_index('model', inplace=True)\n",
    "    df.sort_values(by=['drop_q'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_df_sumscores(key):\n",
    "\n",
    "    sum_score_info = sum_score_stats_multi[key][1]\n",
    "    sum_score_stats_m = sum_score_stats_multi_m[key]\n",
    "    sum_score_stats_ci = sum_score_stats_multi_ci[key]\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        [model_id, sum_score_info[model_id]['drop_n'], int(key)+1]\n",
    "        + [sum_score_stats_ci[model_id]['categorical_accuracy'][0], sum_score_stats_m[model_id]['categorical_accuracy'], sum_score_stats_ci[model_id]['categorical_accuracy'][1]]\n",
    "        + [sum_score_stats_ci[model_id]['val_categorical_accuracy'][0], sum_score_stats_m[model_id]['val_categorical_accuracy'], sum_score_stats_ci[model_id]['val_categorical_accuracy'][1]]\n",
    "        + [sum_score_stats_ci[model_id]['mean_squared_error'][0], sum_score_stats_m[model_id]['mean_squared_error'], sum_score_stats_ci[model_id]['mean_squared_error'][1]]\n",
    "        + [sum_score_stats_ci[model_id]['val_mean_squared_error'][0], sum_score_stats_m[model_id]['val_mean_squared_error'], sum_score_stats_ci[model_id]['val_mean_squared_error'][1]]    \n",
    "        for model_id in natsorted(sum_score_stats_m)\n",
    "    ])\n",
    "\n",
    "    df.columns = [\n",
    "        'model', 'drop_q', 'drop_sum',\n",
    "        'acc_l', 'acc_m','acc_u',\n",
    "        'val_acc_l','val_acc_m','val_acc_u',\n",
    "        'mse_l', 'mse_m', 'mse_u',\n",
    "        'val_mse_l', 'val_mse_m', 'val_mse_u'\n",
    "    ]\n",
    "    df.set_index('model', inplace=True)\n",
    "    df.sort_values(by=['drop_q'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_questions = get_df_questions()\n",
    "df_sumscores_multi = [get_df_sumscores(str(key+1)) for key in range(len(sum_score_stats_multi_m.keys())-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "tab_name = ['pi + q_n'] + ['pi + s_'+item for item in natsorted(sum_score_stats_multi_m.keys())]\n",
    "children = [qgrid.show_grid(df,precision=4) for df in [df_questions]+[df_sumscore for df_sumscore in df_sumscores_multi]]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, tab_name[i])\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def rgb_to_rgba(color_idx, alpha=1):\n",
    "    color = plotly.colors.DEFAULT_PLOTLY_COLORS[color_idx]\n",
    "    return 'rgba'+color[3:-1]+', '+str(alpha)+')'\n",
    "\n",
    "for y_name, y_u_name, y_l_name, title in [\n",
    "    ['val_acc_m', 'val_acc_u', 'val_acc_l', 'Accuracy - validation'],\n",
    "    ['val_mse_m', 'val_mse_u', 'val_mse_l', 'MSE - validation']\n",
    "]:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=go.layout.Title(\n",
    "            text=title,\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "    for plot_idx, (name, data) in enumerate(zip(\n",
    "        ['pi+q_n']+ ['pi + s_'+item for item in natsorted(sum_score_stats_multi_m.keys())],\n",
    "        [df_questions]+[df_sumscore for df_sumscore in df_sumscores_multi]\n",
    "    )):\n",
    "\n",
    "        x = data['drop_q'].tolist()\n",
    "        y = data[y_name].tolist()\n",
    "        y_u = data[y_u_name].tolist()\n",
    "        y_l = data[y_l_name].tolist()\n",
    "        text = data.index.values.tolist()\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x+x[::-1],\n",
    "            y=y_u+y_l[::-1],\n",
    "            fill='toself',\n",
    "            fillcolor=rgb_to_rgba(plot_idx, 0.2),\n",
    "            line_color='rgba(255,255,255,0)',\n",
    "            name=name+' ci'\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x, y=y,\n",
    "            line_color=rgb_to_rgba(plot_idx, 0.6),\n",
    "            mode='lines',\n",
    "            text=text,\n",
    "            name=name\n",
    "        ))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_args = [\n",
    "    ['val_categorical_accuracy', 'Accuracy - validation', [0.7,1], [20,150]],\n",
    "    ['val_categorical_accuracy', 'Accuracy - validation (zoom)', [0.8,1], [50,100]],\n",
    "    ['val_mean_squared_error', 'MSE - validation', [0,10], [20,150]],\n",
    "    ['val_mean_squared_error', 'MSE - validation (zoom)', [0,10], [50,100]],\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(2.5*fig_w, 2.5*fig_h))\n",
    "\n",
    "for my_idx, (my_metric, my_title, my_ylim, my_xlim) in enumerate(plot_args):\n",
    "    plt.sca(ax.flatten()[my_idx])\n",
    "    mod_evaluation.plot_stats_evolution(\n",
    "        questions_stats_best, questions_info_best, my_metric, \n",
    "        models=natsorted(questions_stats_best), recycle=True, \n",
    "        ci=None, label='pi + q_n'\n",
    "    )\n",
    "    \n",
    "    for n in sorted(sum_score_stats_multi):\n",
    "        stats_best, info_best = sum_score_stats_multi[n]\n",
    "        models_list = list(stats_best)\n",
    "\n",
    "        top_idx = np.argsort([info_best[key]['drop_n'] for key in models_list])\n",
    "        models_sorted_n = [models_list[idx] for idx in top_idx]\n",
    "        label = 'pi + s_' + str(n)\n",
    "\n",
    "        mod_evaluation.plot_stats_evolution(\n",
    "            stats_best, info_best, my_metric,\n",
    "            models=models_sorted_n, color='C'+str(n),\n",
    "            recycle=True, ci=None, label=label, alpha_mean=0.6\n",
    "        )\n",
    "        \n",
    "    plt.gca().set_title(my_title)\n",
    "    plt.ylim(my_ylim)\n",
    "    plt.xlim(my_xlim)\n",
    "    if my_idx==0:\n",
    "        plt.legend()\n",
    "    plt.grid(axis='both')\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_models = [models[idx] for idx in range(0,100,10)]\n",
    "# my_cols_exc = [cols_exc[idx] for idx in range(0,100,10)]\n",
    "\n",
    "# stats_train, stats_val, _ =  mod_evaluation.model_train_questions(\n",
    "#     df_train, y_logits_train, df_val, y_logits_val, \n",
    "#     model_ref, my_models, my_cols_exc,\n",
    "#     features_quad=features_quad,\n",
    "#     **model_ref_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train \\tval\\t drop\\t name')\n",
    "# for name_idx, name in enumerate(my_models):\n",
    "    \n",
    "#     print(\n",
    "#         '{:.4f}'.format(stats_train[name]['categorical_accuracy']), \n",
    "#         '\\t{:.4f}'.format(stats_val[name]['categorical_accuracy']), \n",
    "#         '\\t', len(my_cols_exc[name_idx]), '\\t', name\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices from CV and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import HTML\n",
    "# import pandas as pd\n",
    "\n",
    "# print('mean train \\t mean val')\n",
    "# print('train \\t\\t val')\n",
    "\n",
    "# for name_idx, name in enumerate(my_models):\n",
    "    \n",
    "#     display(\n",
    "#         HTML(\n",
    "#             '<style> div>table { display: inline} </style>'\n",
    "#             + '<h4>'+name+'</h4>'\n",
    "#             + pd.DataFrame(mod_evaluation.stats_eval(stats_best)[name]['confusion_matrix']).to_html()\n",
    "#             + pd.DataFrame(mod_evaluation.stats_eval(stats_best)[name]['val_confusion_matrix']).to_html()\n",
    "#             + '<br>'\n",
    "#             + pd.DataFrame(stats_train[name]['confusion_matrix']).to_html()\n",
    "#             + pd.DataFrame(stats_val[name]['confusion_matrix']).to_html()           \n",
    "#             + '<br>'\n",
    "#         )\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
