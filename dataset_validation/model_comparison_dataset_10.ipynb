{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison (dataset)\n",
    "# Drop pi questions with selection metric on train set\n",
    "\n",
    "Reduced models were obtained by the following procedure:\n",
    "* drop all questions from sum score PI\n",
    "* drop next question according to selection metric on **train set**\n",
    "\n",
    "Selection metric criteria:\n",
    "* `ca`: min of mean conditional accuracy\n",
    "* `ca_class`: max of min conditional accuracy\n",
    "* `ca_prod`: max of product conditional accuracy\n",
    "* `mse`: min of mean square error\n",
    "* `mse_class`: min of max conditional mean square error\n",
    "* `xent` min of cross-entropy\n",
    "* `xent_class` min of max cross-entropy\n",
    "\n",
    "http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import mod_evaluation\n",
    "import mod_viewer\n",
    "import mod_helper\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'data/results'\n",
    "\n",
    "model_ref_id = 'linear'\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "train_val_random = None\n",
    "\n",
    "run_types = [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_pre = 'model_'+model_ref_id\n",
    "cache_post = str(n_splits)\n",
    "\n",
    "if train_val_random is not None:\n",
    "    cache_post += '_r'+str(train_val_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = mod_helper.load_data(\n",
    "    results_path,\n",
    "    cache_pre,\n",
    "    cache_post,\n",
    "    mod_evaluation.sort_params_train,\n",
    "    run_types\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat, df_flat_val, df_flat_ca, df_flat_val_ca, info_flat = mod_helper.get_stats_flat(my_data)\n",
    "df_multi, df_multi_val, df_multi_ca, df_multi_val_ca = mod_helper.get_stats_multi(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout set variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_type in my_data:\n",
    "    display(mod_helper.tab_plot_accuracy_multi(\n",
    "        df_multi[run_type],\n",
    "        df_multi_val[run_type]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean accuracy on validation set (cross-validation)\n",
    "\n",
    "* Mean accuracy on validation set (cross-validation) according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "* `ca_class`: max of min conditional accuracy\n",
    "* `mse_class`: min of max conditional mean square error\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_type in my_data:\n",
    "    display(mod_viewer.plot_accuracy_mse(df_flat[run_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean conditional accuracy on validation set  (cross-validation)\n",
    "\n",
    "* Mean conditional accuracy on validation set (cross-validation) according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for run_type in my_data:\n",
    "    display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "        df_flat[run_type],\n",
    "        df_flat_ca[run_type],\n",
    "        info_flat\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean accuracy on holdout set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_type in my_data:\n",
    "    \n",
    "    display(mod_viewer.tab_plot_accuracy(\n",
    "        df_flat[run_type],\n",
    "        info_flat,\n",
    "        df_questions_holdout=df_flat_val[run_type]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean conditional accuracy on validation set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for run_type in my_data:\n",
    "    \n",
    "    display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "        df_flat_val[run_type],\n",
    "        df_flat_val_ca[run_type],\n",
    "        info_flat,\n",
    "        holdout=True\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
