{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection - drop questionnaire items\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5019184/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import mod_data\n",
    "import mod_evaluation\n",
    "import mod_compute\n",
    "import mod_viewer\n",
    "import mod_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/source/all_data_wide.csv'\n",
    "\n",
    "metric_eval = 'val_categorical_accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_path = 'data/results'\n",
    "\n",
    "q_exc = []\n",
    "\n",
    "metric_id = 'categorical_accuracy_class'\n",
    "\n",
    "model_ref_id = 'linear'\n",
    "model_ref_kwargs = {}\n",
    "\n",
    "scaling = None\n",
    "\n",
    "n_splits = 25\n",
    "drop_max = 120\n",
    "\n",
    "train_val_random = 10\n",
    "\n",
    "train = 1\n",
    "test_size = 200\n",
    "\n",
    "save_stats = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model_linear_categorical_accuracy_class_25_r10\n"
     ]
    }
   ],
   "source": [
    "model_ref = mod_compute.models[model_ref_id]\n",
    "cache_pre = 'model_'+model_ref_id\n",
    "cache_post = str(n_splits)\n",
    "\n",
    "if train_val_random is not None:\n",
    "    cache_post += '_r'+str(train_val_random)\n",
    "    \n",
    "cache_sig = mod_evaluation.cache_sig_gen(\n",
    "    metric_id, \n",
    "    cache_pre=cache_pre,\n",
    "    cache_post=cache_post\n",
    ")\n",
    "\n",
    "print('evaluating', cache_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import train and holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(data_path)\n",
    "\n",
    "_, logits = mod_latent.modelPP(df_data[mod_latent.s_latent_ref])\n",
    "\n",
    "df_data[mod_data.logits_ref] = pd.DataFrame(logits)\n",
    "\n",
    "df_data = df_data[df_data['train']==train]\n",
    "\n",
    "df_train, df_val = sk.model_selection.train_test_split(\n",
    "    df_data,\n",
    "    test_size=test_size,\n",
    "    random_state=train_val_random,\n",
    "    shuffle=True,\n",
    "    stratify=np.argmax(np.array(df_data[mod_data.logits_ref]), axis=1)\n",
    ")\n",
    "\n",
    "df_train_val = (df_train, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: (511, 176) \ty train: (511, 5)\n",
      "x val:\t (200, 176) \ty val:\t (200, 5)\n"
     ]
    }
   ],
   "source": [
    "df_x_train, df_y_train, df_x_val, df_y_val = mod_data.load_data_df(df_train_val=df_train_val)\n",
    "\n",
    "print('x train:', df_x_train.shape, '\\ty train:', df_y_train.shape)\n",
    "print('x val:\\t', df_x_val.shape, '\\ty val:\\t', df_y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no scaling applied\n"
     ]
    }
   ],
   "source": [
    "if scaling is not None:\n",
    "    \n",
    "    if scaling=='minmax':\n",
    "        scaler = sk.preprocessing.MinMaxScaler()\n",
    "        \n",
    "    elif scaling=='standard':\n",
    "        scaler = sk.preprocessing.StandardScaler()\n",
    "    \n",
    "    scaler.fit(df_x_train)\n",
    "\n",
    "    columns = df_x_train.columns\n",
    "    \n",
    "    df_x_train = pd.DataFrame(scaler.transform(df_x_train), columns=columns)\n",
    "    df_x_val = pd.DataFrame(scaler.transform(df_x_val), columns=columns)\n",
    "    \n",
    "    print('scaling applied:', scaling)\n",
    "    \n",
    "else:\n",
    "    print('no scaling applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b2826520f5454fbf3449ad03648021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_q\t drop_sum\tval_acc_m (val_acc_min - val_acc_max)\t name\n",
      "\n",
      "1 \t 16 \t\t0.9533 \t(0.8500 - 1.0000) \t\t TEPS_0012\n",
      "2 \t 16 \t\t0.9549 \t(0.7548 - 1.0000) \t\t 1 + ACS_0009\n",
      "3 \t 16 \t\t0.9630 \t(0.8381 - 1.0000) \t\t 2 + PSAS_0013\n",
      "4 \t 16 \t\t0.9605 \t(0.8167 - 1.0000) \t\t 3 + PSAS_0009\n",
      "5 \t 16 \t\t0.9627 \t(0.8667 - 1.0000) \t\t 4 + FSS_0007\n",
      "6 \t 16 \t\t0.9625 \t(0.8500 - 1.0000) \t\t 5 + PSAS_0002\n",
      "7 \t 16 \t\t0.9654 \t(0.8548 - 1.0000) \t\t 6 + ACS_0002\n",
      "8 \t 16 \t\t0.9708 \t(0.8833 - 1.0000) \t\t 7 + PSAS_0011\n"
     ]
    }
   ],
   "source": [
    "stats_best, info_best = mod_compute.stats_seq_drop(\n",
    "    metric_id,\n",
    "    model_ref,\n",
    "    df_x_train,\n",
    "    df_y_train,\n",
    "    q_exc_init=q_exc, \n",
    "    drop_max=drop_max,\n",
    "    n_splits=n_splits,\n",
    "    metric_eval=metric_eval,\n",
    "    **model_ref_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats_val = {}\n",
    "\n",
    "for model_id in info_best:\n",
    "    \n",
    "    q_sel = mod_compute.get_q_sel(info_best[model_id]['q_exc'])\n",
    "\n",
    "    x_train, y_train, x_val, y_val = mod_compute.df_to_model_input(\n",
    "        df_x_train[q_sel], df_y_train, df_x_val[q_sel], df_y_val\n",
    "    )\n",
    "            \n",
    "    _, my_stats =  mod_compute.model_train(\n",
    "        x_train, y_train, x_val, y_val, \n",
    "        model_ref,\n",
    "        **model_ref_kwargs\n",
    "    )\n",
    "    \n",
    "    stats_val[model_id] = [my_stats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_stats:\n",
    "    \n",
    "    if not os.path.isdir(results_path):\n",
    "        os.mkdir(results_path)\n",
    "\n",
    "    results = {\n",
    "        'info': info_best,\n",
    "        'stats': stats_best,\n",
    "        'stats_val': stats_val,\n",
    "    }\n",
    "\n",
    "    mod_evaluation.to_cache(cache_sig, results_path, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {metric_eval: info_best}\n",
    "stats = {metric_eval: stats_best}\n",
    "my_stats_val = {metric_eval: stats_val}\n",
    "    \n",
    "df_questions, df_questions_val = mod_evaluation.get_df_questions(\n",
    "    info, stats, my_stats_val, \n",
    "    ci=True\n",
    ")\n",
    "\n",
    "df_questions_ca, df_questions_val_ca = mod_evaluation.get_df_questions_ca(\n",
    "    info, stats, my_stats_val, \n",
    "    ci=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.plot_accuracy([[\n",
    "    df_questions[metric_eval],\n",
    "    mod_viewer.sort_params_short[metric_eval]\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_viewer.plot_conditional_accuracy(\n",
    "    df_questions[metric_eval], \n",
    "    df_questions_ca[metric_eval]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot holdout results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.plot_accuracy([\n",
    "    [df_questions[metric_eval], mod_viewer.sort_params_short[metric_eval]],\n",
    "    [df_questions_val[metric_eval], 'holdout']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.plot_conditional_accuracy(\n",
    "    df_questions_val[metric_eval], \n",
    "    df_questions_val_ca[metric_eval]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
