{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison (dataset)\n",
    "# Drop pi questions with selection metric on train set\n",
    "\n",
    "Reduced models were obtained by the following procedure:\n",
    "* drop all questions from sum score PI\n",
    "* drop next question according to selection metric on **train set**\n",
    "\n",
    "Selection metric criteria:\n",
    "* `ca`: min of mean conditional accuracy\n",
    "* `ca_class`: max of min conditional accuracy\n",
    "* `ca_prod`: max of product conditional accuracy\n",
    "* `mse`: min of mean square error\n",
    "* `mse_class`: min of max conditional mean square error\n",
    "* `xent` min of cross-entropy\n",
    "* `xent_class` min of max cross-entropy\n",
    "\n",
    "http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import mod_evaluation\n",
    "import mod_viewer\n",
    "import mod_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'data/results'\n",
    "\n",
    "model_ref_id = 'linear'\n",
    "\n",
    "n_splits = 25\n",
    "\n",
    "metrics = mod_evaluation.sort_params_train\n",
    "\n",
    "train_val_random = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_pre = 'model_'+model_ref_id\n",
    "cache_post = str(n_splits)\n",
    "\n",
    "if train_val_random is not None:\n",
    "    cache_post += '_r'+str(train_val_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0\n",
      "Loaded 1\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def get_model_id(model_id):\n",
    "    id_split = model_id.split(' + ')\n",
    "    if len(id_split)>1:\n",
    "        my_model_id = int(id_split[0])\n",
    "    else:\n",
    "        my_model_id = 0\n",
    "        \n",
    "    return my_model_id\n",
    "                        \n",
    "available = {0: [], 1: []}\n",
    "\n",
    "for item in mod_evaluation.list_cache(results_path):\n",
    "    if '_0_' in item:\n",
    "        available[0] += [item]\n",
    "    if '_1_' in item:\n",
    "        available[1] += [item]    \n",
    "\n",
    "my_data = {}\n",
    "\n",
    "for run_type in available:\n",
    "    \n",
    "    info, stats, stats_val = {}, {}, {}\n",
    "\n",
    "    for item in available[run_type]:\n",
    "\n",
    "        for metric_id in metrics:\n",
    "\n",
    "            cache_sig = mod_evaluation.cache_sig_gen(\n",
    "                metric_id, \n",
    "                cache_pre=cache_pre, \n",
    "                cache_post=cache_post\n",
    "            )\n",
    "    \n",
    "            if cache_sig in item:\n",
    "                \n",
    "                run_id = item.split('_')[-1]\n",
    "                \n",
    "                if run_id not in info:\n",
    "                    info[run_id], stats[run_id], stats_val[run_id] = {}, {}, {}\n",
    "                    \n",
    "                if metric_id not in info[run_id]:\n",
    "                    info[run_id][metric_id], stats[run_id][metric_id], stats_val[run_id][metric_id] = {}, {}, {}\n",
    "                    \n",
    "                cache = mod_evaluation.from_cache(\n",
    "                    item, \n",
    "                    results_path\n",
    "                )\n",
    "                \n",
    "                for model_id in cache['info']:\n",
    "                    \n",
    "                    my_model_id = get_model_id(model_id)\n",
    "                    \n",
    "                    info[run_id][metric_id][my_model_id] = cache['info'][model_id]\n",
    "                    stats[run_id][metric_id][my_model_id] = cache['stats'][model_id]\n",
    "                    stats_val[run_id][metric_id][my_model_id] = cache['stats_val'][model_id]\n",
    "\n",
    "    my_data[run_type] = [deepcopy(info), deepcopy(stats), deepcopy(stats_val)]\n",
    "    \n",
    "    print('Loaded', run_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = {0:{}, 1:{}}\n",
    "\n",
    "df_multi, df_multi_val = deepcopy(empty), deepcopy(empty)\n",
    "df_multi_ca, df_multi_val_ca = deepcopy(empty), deepcopy(empty)\n",
    "\n",
    "df_flat, df_flat_val = deepcopy(empty), deepcopy(empty)\n",
    "df_flat_ca, df_flat_val_ca = deepcopy(empty), deepcopy(empty)\n",
    "\n",
    "for run_type in my_data:\n",
    "    \n",
    "    info, stats, stats_val = my_data[run_type]\n",
    "    info_flat, stats_flat, stats_val_flat = {}, {}, {}\n",
    "    \n",
    "    for run_id in info:\n",
    "        \n",
    "        for metric_id in info[run_id]:\n",
    "            \n",
    "            if metric_id not in info_flat:\n",
    "                info_flat[metric_id], stats_flat[metric_id], stats_val_flat[metric_id] = {}, {}, {}\n",
    "\n",
    "            for model_id in info[run_id][metric_id]:\n",
    "                      \n",
    "                if my_model_id not in info_flat:\n",
    "                    info_flat[metric_id][model_id] = info[run_id][metric_id][model_id]\n",
    "                    stats_flat[metric_id][model_id] = stats[run_id][metric_id][model_id]\n",
    "                    stats_val_flat[metric_id][model_id] = stats_val[run_id][metric_id][model_id]\n",
    "                    \n",
    "                else:\n",
    "                    stats_flat[metric_id][model_id] += stats[run_id][metric_id][model_id]\n",
    "                    stats_val_flat[metric_id][model_id] += stats_val[run_id][metric_id][model_id]\n",
    "                    \n",
    "        df_multi[run_type][run_id], df_multi_val[run_type][run_id] = mod_evaluation.get_df_questions(\n",
    "            info[run_id], stats[run_id], stats_val[run_id],\n",
    "            ci=False\n",
    "        )\n",
    "\n",
    "        df_multi_ca[run_type][run_id], df_multi_val_ca[run_type][run_id] = mod_evaluation.get_df_questions_ca(\n",
    "            info[run_id], stats[run_id], stats_val[run_id]\n",
    "        )\n",
    "\n",
    "    df_flat[run_type], df_flat_val[run_type] = mod_evaluation.get_df_questions(\n",
    "        info_flat, stats_flat, stats_val_flat,\n",
    "        ci=True\n",
    "    )\n",
    "\n",
    "    df_flat_ca[run_type], df_flat_val_ca[run_type] = mod_evaluation.get_df_questions_ca(\n",
    "        info_flat, stats_flat, stats_val_flat,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout set variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mod_helper.tab_plot_accuracy_multi(\n",
    "    df_multi[1],\n",
    "    df_multi_val[1]\n",
    "))\n",
    "\n",
    "display(mod_helper.tab_plot_accuracy_multi(\n",
    "    df_multi[0],\n",
    "    df_multi_val[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean accuracy on validation set (cross-validation)\n",
    "\n",
    "* Mean accuracy on validation set (cross-validation) according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "* `ca_class`: max of min conditional accuracy\n",
    "* `mse_class`: min of max conditional mean square error\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29af0188880048b4ae5afcefaf50545a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'fill': 'toself',\n",
       "              'fillcolor': 'rgba(31, 119, 180, 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87decd5b5dd3478387d63435ab23ca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'fill': 'toself',\n",
       "              'fillcolor': 'rgba(31, 119, 180, 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mod_viewer.plot_accuracy_mse(df_flat[1]))\n",
    "\n",
    "display(mod_viewer.plot_accuracy_mse(df_flat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean conditional accuracy on validation set  (cross-validation)\n",
    "\n",
    "* Mean conditional accuracy on validation set (cross-validation) according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab8db0e4af54c3eb324f117e0f81099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'rgba(31, 119, 180, 0.6)'},\n",
       "      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c320f18c5524899975fafda5fb55a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'rgba(31, 119, 180, 0.6)'},\n",
       "      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_flat[1],\n",
    "    df_flat_ca[1],\n",
    "    info_flat\n",
    "))\n",
    "\n",
    "display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_flat[0],\n",
    "    df_flat_ca[0],\n",
    "    info_flat\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean accuracy on holdout set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b3bd13a4f54a27927881f8acd26b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'fill': 'toself',\n",
       "              'fillcolor': 'rgba(3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e83a525a1848caaad93126552cf70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'fill': 'toself',\n",
       "              'fillcolor': 'rgba(3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mod_viewer.tab_plot_accuracy(\n",
    "    df_flat[1],\n",
    "    info_flat,\n",
    "    df_questions_holdout=df_flat_val[1]\n",
    "))\n",
    "\n",
    "display(mod_viewer.tab_plot_accuracy(\n",
    "    df_flat[0],\n",
    "    info_flat,\n",
    "    df_questions_holdout=df_flat_val[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean conditional accuracy on validation set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "Figures: **original dataset** (top), **new dataset** (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2d1b3275f347f4995487474a5eebd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'rgba(31, 119, 180, 0.6)'},\n",
       "      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839cbaedd1b24aef930e508844a46b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(FigureWidget({\n",
       "    'data': [{'line': {'color': 'rgba(31, 119, 180, 0.6)'},\n",
       "      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_flat_val[1],\n",
    "    df_flat_val_ca[1],\n",
    "    info_flat,\n",
    "    holdout=True\n",
    "))\n",
    "\n",
    "display(mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_flat_val[0],\n",
    "    df_flat_val_ca[0],\n",
    "    info_flat,\n",
    "    holdout=True\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
