{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison - linear model\n",
    "# Drop pi questions with selection metric on validation set\n",
    "# Train set - holdout set swapped\n",
    "\n",
    "Reduced models were obtained by the following procedure:\n",
    "* drop all questions from sum score PI\n",
    "* drop next question according to selection metric on **validation set**\n",
    "\n",
    "Selection metric criteria:\n",
    "* `ca_min`: max of min conditional accuracy\n",
    "* `ca_prod`: max of product conditional accuracy\n",
    "* `log_loss_max`: min of max conditional cross-entropy\n",
    "* `log_loss` min of cross-entropy\n",
    "* `mse`: min of mean square error\n",
    "* `mse_max`: min of max conditional mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mod_evaluation\n",
    "import mod_viewer\n",
    "import results_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = 'data/results'\n",
    "\n",
    "questions_stats, questions_info = results_cache.get_questions(\n",
    "    results_folder,\n",
    "    sources=results_cache.sources_rev\n",
    ")\n",
    "\n",
    "df_questions, df_questions_ca = results_cache.get_df_questions(\n",
    "    questions_stats, \n",
    "    questions_info\n",
    ")\n",
    "\n",
    "stats_holdout = mod_evaluation.from_cache(\n",
    "    'questions_drop_pi_holdout_train', \n",
    "    results_folder\n",
    ")\n",
    "\n",
    "df_questions_holdout, df_questions_holdout_ca = results_cache.get_df_questions(\n",
    "    stats_holdout, \n",
    "    questions_info,\n",
    "    ci=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison: mean accuracy on validation set\n",
    "\n",
    "* Mean accuracy on validation set according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)\n",
    "\n",
    "* `ca_min`: max of min conditional accuracy\n",
    "* `ca_prod`: max of product conditional accuracy\n",
    "* `log_loss_max`: min of max conditional cross-entropy\n",
    "* `log_loss` min of cross-entropy\n",
    "* `mse`: min of mean square error\n",
    "* `mse_max`: min of max conditional mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_viewer.plot_simple(\n",
    "    [\n",
    "        [df_questions[item], item]\n",
    "        for item in df_questions\n",
    "    ],\n",
    "    title='Mean validation accuracy',\n",
    "    ci=True,\n",
    "    black_trace=2,\n",
    "    yaxes_range=[0.8, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison: mean conditional accuracy on validation set\n",
    "\n",
    "* Mean conditional accuracy on validation set according to selection metric\n",
    "* Confidence interval estimated by bootstrap method over cross-validation repetitions\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_questions,\n",
    "    df_questions_ca,\n",
    "    questions_info,\n",
    "    model_type_name=results_cache.model_type_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation: mean accuracy on holdout set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.tab_plot_descent_methods_validation(\n",
    "    df_questions, \n",
    "    questions_info,\n",
    "    model_type_name=results_cache.model_type_name,\n",
    "    stats_holdout=stats_holdout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation: mean accuracy on holdout set\n",
    "\n",
    "* Accuracy on holdout set according to selection metric\n",
    "* Holdout accuracy outside confidence interval bounds may indicate (1) model overfitting or (2) data domain shift\n",
    "\n",
    "(clicking on labels adds/removes traces, double-clicking selects single trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_viewer.tab_plot_conditional_accuracy(\n",
    "    df_questions_holdout,\n",
    "    df_questions_holdout_ca,\n",
    "    questions_info,\n",
    "    model_type_name=results_cache.model_type_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
